package openai

import (
	"context"
	"encoding/json"

	"github.com/fabiustech/openai/models"
	"github.com/fabiustech/openai/objects"
	"github.com/fabiustech/openai/routes"
)

// ChatRole is an enum of the various message roles.
type ChatRole string

const (
	// System represents a system message, which helps set the behavior of the assistant.
	System ChatRole = "system"
	// User represents a message from a user, which helps instruct the assistant. They can be generated by the end users
	// of an application, or set by a developer as an instruction.
	User ChatRole = "user"
	// Assistant represents a message from the assistant. Assistant messages help store prior responses. They can also
	// be written by a developer to help give examples of desired behavior.
	Assistant ChatRole = "assistant"
)

// ChatMessage represents a message in a chat completion.
type ChatMessage struct {
	Role    ChatRole `json:"role"`
	Content string   `json:"content"`
}

// TODO(Andy): Support streaming.

// ChatCompletionRequest contains all relevant fields for requests to the chat completions endpoint.
type ChatCompletionRequest struct {
	// Model specifies the ID of the model to use.
	// See more here: https://platform.openai.com/docs/models/overview
	Model models.ChatCompletion `json:"model"`
	// Messages are the messages to generate chat completions for, in the chat format.
	Messages []*ChatMessage `json:"messages"`
	// Temperature specifies what sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
	//output more random, while lower values like 0.2 will make it more focused and deterministic. OpenAI generally
	// recommends altering this or top_p but not both.
	// Defaults to 1.
	Temperature *float64 `json:"temperature,omitempty"`
	// TopP specifies an alternative to sampling with temperature, called nucleus sampling, where the model considers
	// the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10%
	// probability mass are considered. OpenAI generally recommends altering this or temperature but not both.
	// Defaults to 1.
	TopP *float64 `json:"top_p,omitempty"`
	// N specifies many chat completion choices to generate for each input message.
	// Defaults to 1.
	N int `json:"n,omitempty"`
	// Stop specifies up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"`
	// MaxTokens specifies the maximum number of tokens to generate in the chat completion. The total length of input
	// tokens and generated tokens is limited by the model's context length.
	// Defaults to Infinity.
	MaxTokens int `json:"max_tokens,omitempty"`
	// PresencePenalty can be a number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
	// appear in the text so far, increasing the model's likelihood to talk about new topics.
	// Defaults to 0.
	PresencePenalty float32 `json:"presence_penalty,omitempty"`
	// FrequencyPenalty can be a number between -2.0 and 2.0. Positive values penalize new tokens based on their
	// existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	// Defaults to 0.
	FrequencyPenalty float32 `json:"frequency_penalty,omitempty"`
	// LogitBias modifies the likelihood of specified tokens appearing in the completion. Accepts a json object that
	// maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will
	// vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100
	// or 100 should result in a ban or exclusive selection of the relevant token.
	// Defaults to null.
	LogitBias map[string]int `json:"logit_bias,omitempty"`
	// User is a unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	// See more here: https://beta.openai.com/docs/guides/safety-best-practices/end-user-ids
	User string `json:"user,omitempty"`
}

// CreateChatCompletion creates a chat completion for the provided prompt and parameters.
func (c *Client) CreateChatCompletion(ctx context.Context, cr *ChatCompletionRequest) (*ChatCompletionResponse, error) {
	var b, err = c.post(ctx, routes.ChatCompletions, cr)
	if err != nil {
		return nil, err
	}

	var resp = &ChatCompletionResponse{}
	if err = json.Unmarshal(b, resp); err != nil {
		return nil, err
	}

	return resp, nil
}

// ChatCompletionResponse is the response from the chat completions endpoint.
type ChatCompletionResponse struct {
	ID      string                  `json:"id"`
	Object  objects.Object          `json:"object"`
	Created uint64                  `json:"created"`
	Choices []*ChatCompletionChoice `json:"choices"`
	Usage   *Usage                  `json:"usage"`
}

// ChatCompletionChoice represents one of possible chat completions.
type ChatCompletionChoice struct {
	Index        int          `json:"index"`
	Message      *ChatMessage `json:"message"`
	FinishReason string       `json:"finish_reason"`
}
